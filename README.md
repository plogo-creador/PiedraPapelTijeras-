üß† Piedra ‚Äì Papel ‚Äì Tijera ‚Äì Lagarto ‚Äì Spock
Axente Reactivo con Aprendizaxe por Frecuencias
Autor: Adri√°n Rodr√≠guez Sebasti√°n
Curso: MIA
Profesor: @dfleta
 1. Especificaci√≥n do contorno de tarefas

A continuaci√≥n anal√≠zase o contorno segundo os criterios dados en clase:

Propiedade	Valor	Xustificaci√≥n
Observable	Parcialmente observable	O axente s√≥ percibe a xogada do xogador, pero non co√±ece intenci√≥ns nin estratexia.
Axentes	Dous axentes (IA e xogador)	Ambos toman decisi√≥ns independentes, interactuando entre si.
Determinista / Non determinista	Non determinista	Non se pode predicir con certeza a xogada do xogador.
Epis√≥dico / Secuencial	Epis√≥dico	Cada ronda √© independente; non existe un obxectivo a longo prazo m√°is al√° de ga√±ar cada ronda.
Est√°tico / Din√°mico	Est√°tico	O estado do ambiente non cambia mentres o axente est√° a decidir.
Discreto / Continuo	Discreto	S√≥ existen 5 acci√≥ns posibles ben definidas.
Co√±ecido / Desco√±ecido	Co√±ecido	As regras do xogo son fixas e co√±ecidas previamente polo axente.
 2. Tipo de axente e estrutura

O axente implementado √© un Axente Reactivo Baseado en Modelo, xa que:

Percibe a xogada do xogador (‚Äúpercept‚Äù).

Actualiza un estado interno baseado no historial.

Utiliza o estado interno para tomar decisi√≥ns.

A decisi√≥n depende do modelo de aprendizaxe por frecuencias.

 Diagrama do Axente
<p align="center"> <img src="docs/diagramaagente.png" alt="Diagrama del agente" width="600"/> </p>
 Explicaci√≥n da estrutura

Sensores: reciben a xogada do xogador.

Estado Interno: historial de acci√≥ns do xogador.

Funci√≥n de actualizaci√≥n: identifica a xogada m√°is frecuente.

Funci√≥n de decisi√≥n: escolla a acci√≥n que derrota √° m√°is frecuente.

Actuadores: env√≠an a acci√≥n da IA como resposta.

 3. Implementaci√≥n en Python
 Estrutura do proxecto
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ ai_agent.py
‚îÇ   ‚îú‚îÄ‚îÄ rules.py
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îî‚îÄ‚îÄ agent_diagram.png
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .gitignore

 Principios SOLID aplicados

SRP: cada ficheiro cumpre unha √∫nica responsabilidade.

OCP: o sistema √© extensible (engadir novas acci√≥ns e regras sen modificar o n√∫cleo).

M√≥dulos independentes: regras, axente e interface separados.

 IA que aprende por frecuencias
C√≥digo principal da IA
# ai_agent.py
import random
from collections import Counter

class AIAgent:
    def __init__(self, options):
        self.options = options
        self.history = []

    def get_computer_action(self):
        if not self.history:
            return random.choice(self.options)

        freq = Counter(self.history)
        most_common = freq.most_common(1)[0][0]

        rules = {
            "piedra": ["tijera", "lagarto"],
            "papel": ["piedra", "spock"],
            "tijera": ["papel", "lagarto"],
            "lagarto": ["spock", "papel"],
            "spock": ["tijera", "piedra"]
        }

        winning_actions = [action for action, beats in rules.items() if most_common in beats]
        return random.choice(winning_actions)

    def update_history(self, player_action):
        self.history.append(player_action)

4. Extensi√≥n a RPS + Lagarto + Spock

Engad√≠ronse d√∫as novas acci√≥ns ao sistema:

Lagarto

Spock

 Novas regras de victoria

Piedra aplasta Tijera e aplasta Lagarto

Papel cubre Piedra e refuta Spock

Tijera corta Papel e decapita Lagarto

Lagarto envenena Spock e devora Papel

Spock rompe Tijera e vaporiza Piedra

 Adaptaci√≥n da IA

A IA non require modificaci√≥ns estruturais:
a mesma estratexia de frecuencias funciona cos 5 s√≠mbolos.

A clave √© que o axente:

Observa a xogada do xogador.

Aprende cal √© a m√°is frecuente.

Escolla entre as acci√≥ns que lle ga√±an.

Actualiza o historial.

 5. Como executar o programa
python3 src/main.py

 6. .gitignore recomendado
__pycache__/
*.pyc
venv/
.env/
.vscode/

 7. Traballo futuro (opcional)

Engadir aprendizaxe por cadeas de Markov.

Usar redes neuronais simples para predici√≥n.

Crear interface gr√°fica.

 8. Licenza

Licenza libre para uso acad√©mico.